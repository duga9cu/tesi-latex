
\chapter{Dettagli di Implementazione}
\label{sec:internalfunct}
	In questo capitolo si analizzeranno i dettagli del lavoro computazionale che il tracciamento della mappa comporta. 
	La spiegazione, analogamente al flusso di implementazione, segue i seguenti step:
	 \begin{enumerate}
	 \item Acquisizione dei dati;
	 \item Organizzazione del carico computazionale;
	\item Sintesi dei microfoni virtuali mediante convoluzione; 
	\item Scalatura delle ampiezze dei segnali risultanti in funzione del fondo 
	scala specificato nella finestra di configurazione; 
	\item Mirroring dei microfoni virtuali, per garantire la continuità ai bordi 
	della mappa; 
	\item Copertura dell'intera area della mappa con superfici di forma 
	triangolare (\emph{meshing}); 
	\item Interpolazione dei dati e filtraggio in bande d'ottava; 
	\item Applicazione della \emph{color-map} in relazione all'opzione di \emph{auto-scaling};
	\item Sincronizzazione audio-video.
	\end{enumerate}

	Nei capitoli precedenti si è già analizzato lo step della sintesi dei microfoni virtuali mediante convoluzione (punto 2). Si procederà di seguito con la spiegazione in dettaglio degli altri passaggi.


	
	\section{Acquisizione dei dati in ingresso}
	Per il corretto funzionamento del \emph{plug-in}, abbiamo visto, nel capitolo precedente alla sezione \ref{sec:confdlg}, che sono necessari i seguenti dati in ingresso:
	\begin{itemize}
  \item Una \textbf{registrazione multicanale} effettuata con l'array microfonico in uso,
  \item Un \textbf{videoclip dell'ambiente di background},
  \item Una \textbf{matrice di filtri} in grado di sintetizzare i microfoni virtuali a partire dalle capsule dell'array,
  \item le \textbf{coordinate} in \emph{pixel} delle posizioni dei singoli microfoni virtuali sulla immagine di background.
\end{itemize}
	Di tutti questi dati, viene controllata la coerenza in fase di configurazione\footnote{ ancora come già detto al capitolo  \ref{sec:confdlg}}.

	Per quanto riguarda la \textbf{registrazione multicanale} direttamente proveniente dall'array, è lo stesso host \audacity a farsi carico di rendere disponibile la registrazione al \emph{plug-in} tramite l'utilizzo di apposite \emph{routines}.

	La seconda componente fondamentale per la costruzione della mappa acustica è il \textbf{videoclip di background}. 
	Essendo a conoscenza del \emph{path} nel \emph{filesystem}, fornitoci in fase di configurazione dall'utente, utilizziamo alcune \emph{routines} basate sulle \emph{API} fornite dalla suite \textsf{ffmpeg} per acquisire il video, salvarlo in memoria diviso in singoli frames (quindi immagini di tipo bitmap o per meglio dire \textsf{wxBitmap}). 
	A tale scopo è stata creata una struttura dati complessa composta da due classi fondamentali:
	
	\begin{itemize}
	  \item la classe \textsf{Video} che si occupa di modellizzare il video finale di output,
	  \item la classe \textsf{VideoFrame} la quale implementa la struttura logica dei singoli frame del video e contiene tutte le informazioni di pressione, livelli \emph{SPL}, le varie bitmap di sfondo e di mappe di colore per ogni banda frequenziale, nonché le informazioni temporali di \emph{timestamp} riguardanti la sincronizzazione tra audio e relativa mappa acustica e video.
\end{itemize}

	Per quanto riguarda la \textbf{matrice di filtri}, abbiamo visto\footnote{al capitolo \ref{sec:}} che viene fornita al plugin sotto forma di file .wav, che viene caricato in memoria \emph{heap} durante la fase di \emph{precalcolo}\footnote{dove ne parli?} per poi venire \emph{deallocato} al momento della presentazione dei risultati. 

	Infine le\textbf{ coordinate} dei microfoni virtuali vengono lette dal file \textsf{xml} attraverso le \emph{routines} della libreria \textsf{tinyXml} e salvate in una struttura appositamente creata chiamata \textsf{MikesCoordinates}.

	
	
	
	\section{Organizzazione del carico computazionale}
	A questo punto, avendo caricato in memoria tutti i dati necessari, è possibile iniziare l'elaborazione vera e propria. 
	Tutta la parte che segue riguarda una descrizione puramente implementativa, che chiameremo fase di \emph{precalcolo}, la quale dal punto di vista dell'utente corrisponde a una semplice \emph{progress bar} che comunicherà lo stato di avanzamento del programma durante l'elaborazione.
	
	Trattandosi dell'elaborazione di un video, si è pensato di parallelizzare il \emph{precalcolo} in modo da approfittare della ripetitività del flusso di lavoro che deve analizzare i dati dividendoli in piccoli \emph{chunk} temporali corrispondenti ai frame video che si vorranno ottenere.
	Si procede dunque all'allocazione dinamica di svariati \emph{thread} paralleli che concorreranno alla formazione del video finale, calcolando un singolo frame ciascuno.
	Per l'implementazione di questa parallelizzazione si è utilizzata la classe fornita da \textsf{wxWidgets} chiamata \textsf{wxThread} in modo da mantenere lo stile di programmazione multipiattaforma prefissato. 
	Si tratta di una implementazione che aderisce fedelmente allo standard \textsf{POSIX}, in modo da non stravolgere le abitudini implementative del programmatore, ma che contiene alcune classi avanzate in grado di semplificare abbastanza la gestione delle corse critiche e della mutua esclusione tra gli accessi concorrenti.
	
	Ogni \emph{thread} dunque si incaricherà di memorizzare in una sua struttura apposita i dati riguardanti il suo \emph{chunk} di audio e procederà con l'esecuzione di tutti i passi che verranno descritti nelle prossime sezioni.
	Alla fine del proprio iter procedurale andrà a salvare i suoi risultati in una struttura \textsf{VideoFrame} che verrà aggiunta (nella posizione coerente con il numero di frame assegnato al thread stesso) alla struttura condivisa \textsf{video}, naturalmente rispettando i principi che regolano l'accesso concorrente.	
	
	Il main thread, dopo aver \emph{aspettato} tutti gli altri thread\footnote{ operazione che nello standard \textsf{POSIX} prende il nome di \textsf{join}} libererà la memoria inutilizzata e aggiungerà informazioni globali derivanti dall'analisi dell'intero video (come il massimo assoluto di fondoscala).
	
	
	
	
	\section{Livello di fondo scala}
	Il livello di fondo scala ($FS$) specificato dall'utente tramite la finestra di configurazione del \emph{ plug-in}, è inteso come livello massimo con il quale fornire un riferimento di pressione acustica al sensore.	
	Posto che non sono ancora stati scoperti metodi efficaci per la taratura degli array microfonici, si dimostra comunque necessario effettuare una sorta di \emph{adeguamento} dei risultati per ottenere una mappa di significato fisico dal punto di vista quantitativo dei livelli sonori e non solo una corretta interpretazione proporzionale di questi.
	Il problema è di natura elettroacustica e di elaborazione digitale dei segnali audio: il file \emph{wav} come altri standard di questo tipo, rappresentano il suono analogico attraverso il campionamento e la quantizzazione\footnote{appendice \ref{sec:}}, per poi assegnare a ogni \emph{sample } un valore compreso tra -1 e 1.
	Ora, risulta chiaro che questi non possono essere direttamente la misura dei valori di pressione acustica incidente sulla capsula microfonica, ma sono invece rappresentativi del segnale elettrico in cui essa viene tradotta dal sensore.
	
	Per ritradurre l'informazione in pascal e ottenere quindi un informazione dal significato acustico preciso, è necessaria una operazione di \emph{taratura} dello strumento che permetta di misurare un segnale a frequenza e SPL note a priori, in modo da poter mappare la risposta del sensore ed effettuare una opportuna calibrazione applicando un guadagno in ingresso.
	Non si è ancora riusciti a riprodurre questo procedimento su un sensore cosi complesso come un array microfonico, perciò si è escogitato uno stratagemma intuitivo che permetta di effettuare questo adeguamento facilmente, anche se senza pretesa di precisione assoluta.
	Non si tratta infatti di un'operazione di taratura dello strumento, se non di un semplice\emph{adeguamento} dei risultati a una condizione di \emph{verosimiglianza fisica}.
	
	Si rende necessario quindi misurare, durante la registrazione con l'array microfonico, un segnale di riferimento, per esempio attraverso un fonometro calibrato, dal quale risalire al livello SPL di fondoscala della registrazione.
	Una volta individuato il picco massimo in dB sarà sufficiente fornire questo valore al \emph{plug-in} in fase di configurazione e il software si occuperà di computare questo semplice adeguamento dei risultati senza variare i rapporti energetici tra i contenuti frequenziali alle varie bande o nelle diverse zone del campo acustico.
	
		
	
	
	\section{\emph{Mirroring} dei microfoni virtuali}
	\label{sec:mirroring}
	La registrazione effettuata con un array sferico non produce una mappa dei livelli sonori come proiezione cilindrica di una mappa sferica, come ad esempio una mappa ottenibile con un procedimento simile a quello usato per la rappresentazione della superficie terrestre sul planisfero. 
Muovendo dall'esempio del planisfero, è noto che, essendo la terra sferica, muovendosi lungo i bordi del planisfero si avrà una certa continuità della mappa, ovvero uscendo da nord si rientrerà da sud, uscendo da est si rientrerà da ovest etc. 
Nel caso particolare di tracciamento della mappa sonora,si è proceduto prolungando la mappa stessa oltre i propri bordi. Per fare ciò si è effettuata una estensione della mappa \emph{specchiando} i punti relativi ai microfoni virtuali sintetizzati seguendo lo schema proposto nel documento~\cite{spheric-soundfield} e riportato in Figura~\ref{fig:spheric}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% FIGURA MIRRORING %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
  \centering
  \includegraphics[width = 11cm]{img/spheric.pdf}
  \caption{Rappresentazione schematica del processo di \emph{mirroring}, necessario a garantire la 
continuità ai bordi della mappa.}
  \label{fig:spheric}
\end{figure}

	Dalla figura si osserva che i quattro quadranti in cui può essere scomposta la fotografia sferica, A, B, C e D, debbano essere \emph{copiati e specchiati}  verticalmente e/o orizzontalmente attorno alla fotografia, per imporre la condizione di continuità; più precisamente si è proceduto copiando le posizioni e i livelli registrati dai microfoni virtuali in modo da riuscire a coprire con delle \emph{mesh} triangolari anche i bordi della mappa, così come meglio spiegato nel prossimo paragrafo.
	
	
	
	
	\section{\emph{Meshing} della superficie mediante l'operazione di triangolazione di \emph{Delaunay}}
	\label{sec:delaunay}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% FIGURA CAPSULE %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
  \centering
  \includegraphics[width = 13cm]{img/capsules.pdf}
  \caption{Direzioni di puntamento delle capsule dell'\eigen sovrapposte ad una foto panoramica del Teatro alla Scala di Milano, svolta secondo lo schema di proiezione visto nella sezione~\ref{sec:mirroring}.}
  \label{fig:capsules}
\end{figure}
	Per eseguire l'interpolazione di una serie di livelli noti di una funzione in due variabili, l'ascissa e l'ordinata, occorre suddividere l'intero piano in aree più piccole di forma poligonale, aventi come vertici tre o più coppie di coordinate in cui i livelli siano noti; 
	occorre poi ipotizzare, per ciascuna di queste aree, che i livelli ai vertici facciano tutti parte di un'unica funzione notevole, ed infine calcolare i valori che tale funzione assume in tutti i punti compresi quelli di ognuna delle sottoaree in cui si è suddivisa la superficie. 
	Questo primo passaggio è detto \emph{meshing} della superficie, mentre le sottoaree che vanno a ricoprire l'intera superficie prendono il nome di \emph{mesh}. 
	
	Il primo passo da fare è stabilire il metodo da seguire per determinare la griglia, ovvero la forma geometrica delle \emph{mesh} in cui l'intero piano xy andrà suddiviso. 
La prima ipotesi potrebbe essere la suddivisione in aree di forma quadrata (come i meridiani e i paralleli del planisfero): questa ipotesi è valida se i microfoni virtuali, che costituiscono la griglia dei punti in cui i livelli sonori sono noti, sono disposti su una griglia uniforme. 

	Come si può notare dalla Figura~\ref{fig:capsules}, le posizioni delle capsule dell'\eigen purtroppo non lo sono, e se anche così fosse, lo sviluppo del plug-in sotto l'ipotesi di griglia uniforme avrebbe portato alla realizzazione di un software utilizzabile per un numero limitato di disposizioni dei microfoni virtuali. 
	
	La seconda ipotesi è quella di suddividere l'intero piano xy in superfici di forma triangolare; 
	in questo modo, anche nel caso in cui le posizioni dei microfoni fossero disposte nella maniera più casuale possibile, si potrà sempre determinare un insieme continuo di triangoli che copra l'intera superficie. 
	Nel 1925 è stato, infatti, dimostrato che ogni superﬁcie può essere partizionata attraverso superfici di forma triangolare, ma questo può richiedere un numero inﬁnito di triangoli.
	Per effettuare questa operazione si è scelto quindi di utilizzare una triangolazione particolare detta di \emph{Delaunay} che è definita come segue:
	
	\emph{Una triangolazione di un insieme ﬁnito di punti $P \subset R^2$ viene detta di Delaunay se il cerchio circoscritto ad ogni triangolo è vuoto, ovvero nessun punto di $P$ vi giace all’interno}. 
	
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% FIGURA delaunay %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
  \centering
  \includegraphics[width = 8cm]{img/delaunay.png}
  \caption{Descrizione della proprietà dei triangoli formati con l'algoritmo di \emph{Delaunay}.}
  \label{fig:delaunay}
\end{figure}
	
	Sono diversi gli algoritmi che consentono di determinare, dato un insieme di punti sparsi su un piano, la triangolazione di Delaunay. 
	I principali, con complessità differente in funzione del numero di punti da triangolare, sono: 
\begin{itemize}
\item  l'algoritmo incrementale; 
\item  l'algoritmo dividi et impera; 
\item  l'algoritmo Convex Hull. 
\end{itemize}	
	Per il presente lavoro di tesi si è deciso di utilizzare una libreria esterna, denominata  \textsf{Triangle++}, per implementare l'algoritmo di \emph{Delaunay}, essendo l’implementazione particolarmente impegnativa. 
\textsf{Triangle++} è il nome del wrapper \textsf{C++} della libreria \textsf{C} \textsf{Triangle}\footnote{Si veda \cite{triangle}}; esso fornisce la definizione di una classe molto semplice da utilizzare, la quale, dato un insieme di punti di cui si vuole conoscere la triangolazione, svolge autonomamente il calcolo delle mesh facendo uso di tutti e tre gli algoritmi sopraccitati. 	
	
	
	
	\section{Interpolazione dei valori acquisiti e filtraggio in bande d'ottava}
	
	Quando si sia calcolata una ipotetica partizione del piano $xy$ in \emph{mesh} di forma triangolare, all'interno di ciascuna delle quali ci interessi conoscere i valori assunti dalla funzione incognita dei livelli sonori, occorre scegliere, tra le tante possibili, una e una sola funzione da utilizzare come stima approssimativa di quella incognita, imponendo che soddisfi i livelli noti presenti ai tre vertici della mesh. 
La soluzione più semplice è quella di scegliere come funzione approssimante l'equazione di un piano passante per tre punti generici.  
Il metodo di interpolazione che ne deriva va sotto il nome di \emph{interpolazione bilineare}. 

Sapendo che l'equazione generica di un piano può essere scritta nella forma: 
	
	\begin{equation}
	\label{eq:plane}
		z=A\cdot x + B\cdot y + C
	\end{equation}

	e chiamando ($x_{1i}$, $y_{1i}$, $z_{1i}$), ($x_{2i}$, $y_{2i}$, $z_{2i}$) e ($x_{3i}$, $y_{3i}$, $z_{3i}$) le coordinate degli unici tre punti noti della funzione incognita dei livelli sonori, dove i valori $z_{1i}$, $z_{2i}$ e $z_{3i}$ corrispondono proprio ai livelli misurati in corrispondenza dei tre vertici della i-esima mesh, l'imposizione del passaggio del piano per i tre punti corrisponde all'equazione matriciale: 
		\begin{equation}
		\label{eq:planefor3points}
		\left(\begin{array}{c c c }
		x_{1i} & y_{1i} & 1 \\
		x_{2i} & y_{2i} & 1 \\
		 x_{3i} & y_{3i} & 1 \\
		 \end{array}\right)
		 \cdot
		 \left(\begin{array}{c}A \\B \\C\end{array}\right)	
		 =
		 \left(\begin{array}{c}z_{1i} \\z_{2i} \\C\end{array}\right)
 	\end{equation}
	
%	che, attraverso semplici manipolazioni, può essere riscritta come:
%
%	\begin{equation}
%	\left(\begin{array}{c}A \\B \\C\end{array}\right)
%	=
%	\frac{1}{\det (M)}
%	\cdot
%	\left(\begin{array}{c}
%		z_{1i}\cdot (y_{2i} - y_{3i}) + z_{2i}\cdot (y_{3i} - y_{1i}) + z_{3i}\cdot (y_{1i} - y_{2i})  \\
%		z_{1i}\cdot (x_{3i} - x_{2i}) + z_{2i}\cdot (x_{1i} - x_{3i}) + z_{3i}\cdot (x_{2i} - x_{1i})  \\
%		z_{1i}\cdot (x_{2i}\cdot y_{3i} - x_{3i}\cdot y_{2i} ) + z_{2i}\cdot (x_{2i}\cdot y_{1i} - x_{1i}\cdot y_{3i} ) + z_{3i}\cdot (x_{1i}\cdot y_{2i} - x_{2i}\cdot y_{1i} ) 
%	\end{array}\right)	
%	\end{equation}

	L'equazione \ref{eq:planefor3points} rappresenta esattamente l'approccio utilizzato dalla classe \textsf{TriangularMesh}, definita all'interno del codice del \emph{plug-in} progettato, per il calcolo dei coefficienti $A$, $B$, $C$ e $\det (M)$ che servono per l'interpolazione. 
	Quando si desidererà conoscere il livello sonoro in una certa posizione di coordinate (in pixel) che cadono entro una certa \emph{mesh}, si dovrà chiamare la funzione membro dell'oggetto che rappresenta la \emph{mesh} di interesse e che, implementando la \ref{eq:planefor3points}, ci restituirà il valore interpolato.
	 
	Questa ipotesi sarebbe corretta nel caso in cui ad ogni vertice di ciascuna mesh fosse possibile associare un unico livello sonoro, ma ciò non può avvenire. 
	Come si è visto nei precedenti capitoli, infatti, il \emph{plug-in} consente di effettuare una analisi in bande d'ottava\footnote{si veda la sezione \ref{sec:band} per la definizione delle bande utilizzate} dei livelli sonori, ovvero si avranno non una sola mappa dei livelli, bensì tante mappe quante sono le possibili bande d'ottava selezionabili. 
	Per risolvere occorre usare come terza "coordinata" di ciascun vertice il numero del microfono virtuale posizionato nelle stesse coordinate del vertice (anzichè il livello sonoro) e memorizzare una matrice dei livelli sonori tale che, confrontando il numero del microfono virtuale con il numero identificativo della banda selezionata, fornisca il livello sonoro corretto per la determinazione dei coefficienti $A$, $B$, $C$ e $\det (M)$ necessari per la costruzione della mappa della particolare banda selezionata. 
	Il processo di filtraggio è stato affrontato in modo trasparente grazie all'utilizzo di una classe già progettata per la suite di plug-in \textsf{Aurora} per \audacity\footnote{si veda \cite{aurora}} la quale già disponeva di tutte le funzioni membro necessarie ad implementare i filtraggi in bande d'ottava, come definiti secondo norma IEC-1260.
	Infine, dopo l’operazione di filtraggio, è necessario creare un \emph{frame audio} cioè una mappa statica che rappresenti il contenuto del campo acustico nell'intervallo non infinitesimo di un frame\footnote{selezionato dall'utente come al paragrafo \ref{sec:framelength}} del video che si sta andando a costruire; occorre qui un calcolo del valore \emph{$L_{EQ}$}\footnote{definito alla sezione \ref{sec:leq}} dei segnali filtrati. 





	\section{Mappatura con scale cromatiche e \emph{auto-scaling}}
	
	La scala di colore che si utilizza è una funzione in una incognita a tre variabili dipendenti: l'incognita è il livello sonoro in un dato punto ed i tre valori di uscita della funzione coincidono con i tre canali $R$ ($red$), $G$ ($green$) e $B$ ($blue$) di una interfaccia video. 
	A seconda della scala di colore disederata, sarà utilizzata una \emph{routine} diversa per determinare il valore $RGB$ del pixel interessato, il quale sarà successivamente inserito in una \emph{mappa RGB} (una bitmap, appunto). 
	Per assegnare ai valori di $SPL$ un valore $RGB$ è necessario stabilire gli estremi SPL da rappresentare per poi successivamente effettuare tutta la scalatura tra i livelli in modo proporzionale. Infatti le relazioni tra i valori $R$, $G$ e $B$ e il livello che si vuole mappare sono parametrizzate in funzione dei valori di minimo e massimo rappresentabili. 
	Non si tratta di un operazione banalissima in quanto per definire univocamente questi valori è necessario analizzare tutto il video alla ricerca degli estremi.
	Questo comporta la perdita della possibilità di lavorare in tempo reale con una registrazione (a meno di accontentarsi di approssimazioni\footnote{si potrebbe per esempio... }), sarà invece necessario analizzare un audio pre-registrato.
	Dopo aver calcolato tutti i livelli corrispondenti ai microfoni virtuali, filtrati per ogni banda, ed effettuata questa operazione per ogni singolo frame\footnote{ cioè in seguito al precalcolo descritto in questo capitolo}, il programma è in grado di ottenere e salvare in una struttura dati i valori di $SPL$ massimi e minimi per ogni banda e per ogni frame.
	In questo modo sarà possibile individuare gli estremi cercati.
	
	Con la funzione di \textsf{autoscale} si modificano i valori minimi e massimi entro i quali verrà adattata la scala colorata in base alla banda frequenziale selezionata dall'utente.
	
	Nel caso in cui sia stata abilitata la funzione \textsf{auto-scale}\footnote{con il \emph{checkbox} descritto nella sezione \ref{sec:options}}, saranno modificati i valori $RMS$ di minimo e massimo dei segnali prodotti dai microfoni virtuali limitando la ricerca in ogni frame ai soli valori riguardanti la banda selezionata; 
	se il livello minimo così calcolato dovesse essere inferiore a quello di soglia stabilito dall'utente\footnote{si veda il paragrafo \ref{sec:confdlg}}, si considererà come valore minimo il valore di soglia. 
	
	Nel caso in cui invece la funzione \textsf{auto-scale} sia disabilitata, il livello minimo verrà assunto pari a quello di soglia inserito dall'utente, mentre quello massimo verrà assunto pari al massimo assoluto tra i livelli $RMS$ prodotti dai vari microfoni virtuali in tutte le bande e fra tutti i singoli frame del video di output. 
	
	
	
	
	\section{Esportazione dei risultati}
	La struttura dati \textsf{Video} conterrà le seguenti informazioni generali:
	\begin{itemize}
	  \item il livello di trasparenza delle bande colorate rispetto allo sfondo,
	  \item \emph{l'aspect ratio} proprio del video (in \emph{pixel}),
	  \item il numero totale dei frame del video
	  \item i valori di $SPL$ massimo e minimo assoluti per ogni banda
	\end{itemize}
	Inoltre conterrà una struttura di tipo \textsf{hash-map} contenenti i singoli \textsf{VideoFrame} contrassegnati dal numero di frame come chiave unica.
	Ogni \textsf{VideoFrame} dal canto suo, conterrà le informazioni seguenti:
	\begin{itemize}
	\item \emph{timestamp} del frame ottenuta dal progetto \audacity al momento del salvataggio del \emph{chunck} audio,
	  \item Immagine \textsf{bitmap} di background corrispondente al frame più vicino alla timestamp di cui sopra,
	  \item una matrice contenente per ogni canale \emph{virtuale} registrato (cioè per ogni microfono virtuale ricavato dai filtri di inversione) i valori $SPL$ filtrati secondo le 10 bande frequenziali con l'aggiunta del valore filtrato A ($dB(A)$) e del filtraggio lineare in banda stretta\footnote{???}
	  \item una matrice contenente il valore $SPL$ per ogni pixel dell'immagine, ottenuto a seguito dell'interpolazione dei valori dei singoli microfoni virtuali.
	  \item i valori di massimo e minimo $SPL$ per ognuna delle 10 bande e per gli altri 2 filtraggi descritti ($dB(A)$ e $lineare$).
	\end{itemize}
	
	A questo punto, ogni \textsf{videoFrame} può essere salvato su disco rigido con una funzione della libreria \textsf{wxWidgets} che garantirà quindi un'operazione ad hoc funzionante su ogni piattaforma\footnote{per ottenere il video --- ???}.
	[...]\\
	
	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{table}[htbp]
%   \centering
%   \begin{tabular}{l r r r r r r r}
%      $f_\textrm{cb}$ &   125 &   250 &   500 &  1000 &  2000 &  4000 &  8000 \\
%      \hline
%      \emph{SdF}      & -7.96 & -7.80 & -7.56 & -7.34 & -6.92 & -5.88 & -3.92 \\
%      \emph{SdP 1}    & -3.66 & -3.53 & -3.31 & -3.15 & -2.70 & -1.61 &  0.74 \\
%      \emph{SdP 2}    & -1.03 & -0.91 & -0.72 & -0.65 & -0.25 &  0.69 &  3.26 \\
%      \emph{SdP 3}    &  0.23 &  0.36 &  0.53 &  0.57 &  0.95 &  1.80 &  4.57 \\
%      \emph{SdP 4}    &  1.63 &  1.72 &  1.86 &  1.80 &  2.19 &  3.05 &  6.61 \\
%      \emph{SdP 5}   
%   \end{tabular}
%   \caption{$C_{80}$ per un ascoltatore situato a met\`a sala: confronto tra stato di fatto e gli stati di progetto elaborati}
%   \label{tab:c80mid}
%\end{table}

%\begin{table}[htbp]
%   \centering
%   \begin{tabular}{l r r r r r r r}
%      $f_\textrm{cb}$ &   125 &   250 &   500 &  1000 &  2000 &  4000 &  8000 \\
%      \hline
%      \emph{SdF}      & -7.58 & -7.45 & -7.22 & -7.04 & -6.65 & -5.68 & -3.73 \\
%      \emph{SdP 1}    & -5.85 & -5.72 & -5.49 & -5.33 & -4.87 & -3.81 & -1.28 \\
%      \emph{SdP 2}    & -2.29 & -2.17 & -1.95 & -1.85 & -1.42 & -0.42 &  2.33 \\
%      \emph{SdP 3}    & -1.29 & -1.12 & -0.88 & -0.81 & -0.33 &  0.75 &  4.04 \\
%      \emph{SdP 4}    &  0.79 &  0.90 &  1.08 &  1.03 &  1.49 &  2.49 &  6.50 \\
%      \emph{SdP 5}   
%   \end{tabular}
%   \caption{$C_{80}$ per un ascoltatore situato in fondo alla sala: confronto tra stato di fatto e gli stati di progetto elaborati}
%   \label{tab:c80far}
%\end{table}
